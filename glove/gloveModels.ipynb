{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from load import load_preprocessed_data, label2emotion\n",
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import argparse\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Concatenate, Reshape, GRU, Bidirectional, Dropout, Conv1D, Flatten, MaxPool1D, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LeakyReLU\n",
    "np.random.seed(7)\n",
    "from keras.models import Model\n",
    "\n",
    "#Keras Callback to compute F1 score for each epoch\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        r = self.model.predict(x)\n",
    "        getMetrics(r, y)\n",
    "\n",
    "\n",
    "validationDataPath = \"\"\n",
    "trainDataPath = \"\"\n",
    "testDataPath = \"\"\n",
    "solutionPath = \"\"\n",
    "gloveDir = \"\"\n",
    "\n",
    "trainDataPath = '../data/starterkit/train.txt'\n",
    "#testDataPath = '../data/starterkit/devwithoutlabels.txt'\n",
    "groundTruthTestData =  \"../data/test.txt\"\n",
    "validationDataPath = '../data/devsetwithlabels.txt'\n",
    "solutionPath = 'glovelstmtest.txt'\n",
    "gloveDir = '../data/'\n",
    "ssweDir = '../data/'\n",
    "\n",
    "\n",
    "NUM_CLASSES = 4 \n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 64\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 200\n",
    "LSTM_DIM = 300\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "DROPOUT = 0.2\n",
    "\n",
    "\n",
    "def create_solution_file(model,u_testSequences):\n",
    "    u_testData = pad_sequences(u_testSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions = model.predict(u_testData, batch_size=BATCH_SIZE)\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "\n",
    "    with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "        fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')\n",
    "        with io.open(groundTruthTestData, encoding=\"utf8\") as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "                fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "    print(\"Completed. Model parameters: \")\n",
    "    print(\"Learning rate : %.3f, LSTM Dim : %d, Dropout : %.3f, Batch_size : %d\"\n",
    "          % (LEARNING_RATE, LSTM_DIM, DROPOUT, BATCH_SIZE))\n",
    "    return\n",
    "\n",
    "def create_solution_file2(model,u_testSequences):\n",
    "    u_testData = pad_sequences(u_testSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions = model.predict([u_testData,u_testData], batch_size=BATCH_SIZE)\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "\n",
    "    with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "        fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')\n",
    "        with io.open(groundTruthTestData, encoding=\"utf8\") as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "                fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "    print(\"Completed. Model parameters: \")\n",
    "    print(\"Learning rate : %.3f, LSTM Dim : %d, Dropout : %.3f, Batch_size : %d\"\n",
    "          % (LEARNING_RATE, LSTM_DIM, DROPOUT, BATCH_SIZE))\n",
    "    return\n",
    "\n",
    "#Create the embedding matrix\n",
    "def getEmbeddingMatrix(wordIndex, preEmbeddingDir, preEmbeddingFile):\n",
    "    embeddingsIndex = {}\n",
    "    with io.open(os.path.join(preEmbeddingDir, preEmbeddingFile), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embeddingVector = np.array([float(val) for val in values[1:]])\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "        else:\n",
    "            oov = [np.random.normal(size = EMBEDDING_DIM)]\n",
    "            oov /= np.linalg.norm(oov)\n",
    "            embeddingMatrix[i] = oov\n",
    "\n",
    "    return embeddingMatrix\n",
    "\n",
    "\n",
    "#Compute the micro F1 score, the average accuracy, the micro precision and the micro recall\n",
    "def getMetrics(predictions, ground):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification\n",
    "    \"\"\"\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "\n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "\n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "\n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "\n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))\n",
    "\n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()\n",
    "\n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "\n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "\n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "\n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "\n",
    "    return accuracy, microPrecision, microRecall, microF1\n",
    "\n",
    "\n",
    "#Bidirectional LSTM Model\n",
    "def model1(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Bidirectional(LSTM(LSTM_DIM)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#GRU Model\n",
    "def model2(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(GRU(128))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES * 8, activation='relu'))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#CNN Model\n",
    "def model3(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Conv1D(64, 3, padding='same'))\n",
    "    model.add(Conv1D(32, 3, padding='same'))\n",
    "    model.add(Conv1D(16, 3, padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(180, activation='relu'))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#CNN-LSTM Model\n",
    "def model4(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(LSTM(LSTM_DIM))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#LSTM-CNN Model\n",
    "def model5(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(LSTM(LSTM_DIM, return_sequences=True))\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#CNN-BiLSTM Model\n",
    "def model6(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Bidirectional(LSTM(LSTM_DIM)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#BiLSTM-CNN Model\n",
    "def model7(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Bidirectional(LSTM(LSTM_DIM, return_sequences=True)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#BiLSTM Model without dropout after embedding layer\n",
    "def model8(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Bidirectional(LSTM(LSTM_DIM, dropout=0.6)))\n",
    "    model.add(Dropout(0.9))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dropout(0.9))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#CNN-GRU Model\n",
    "def model9(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(GRU(LSTM_DIM))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#GRU-CNN Model\n",
    "def model10(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(GRU(LSTM_DIM, return_sequences=True))\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#lstm Model for first layer refer to \"A Sentiment-and-Semantics-Based Approach for Emotion Detection in Textual Conversations\"\n",
    "def modelLstm(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "#FC Model for second layer refer to \"A Sentiment-and-Semantics-Based Approach for Emotion Detection in Textual Conversations\"\n",
    "def modelFC(embeddingMatrix):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NUM_CLASSES, input_dim=64 * 2))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "from keras.layers import *\n",
    "\n",
    "def custom_arch(gloveEmbeddingMatrix,ssweEmbeddingMatrix):\n",
    "\n",
    "    \n",
    "    gloveembeddingLayer = Embedding(gloveEmbeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[gloveEmbeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model_glove = Sequential()\n",
    "    model_glove.add(gloveembeddingLayer)\n",
    "    model_glove.add(GRU(LSTM_DIM, return_sequences=True))\n",
    "    model_glove.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model_glove.add(MaxPool1D(2))\n",
    "    model_glove.add(Flatten())\n",
    "    model_glove.add(Dropout(0.4))\n",
    "    model_glove.add(Dense(64, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model_glove.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    ssweembeddingLayer = Embedding(ssweEmbeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[ssweEmbeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "        \n",
    "    model_sswe = Sequential()\n",
    "    model_sswe.add(ssweembeddingLayer)\n",
    "    model_sswe.add(GRU(LSTM_DIM, return_sequences=True))\n",
    "    model_sswe.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model_sswe.add(MaxPool1D(2))\n",
    "    model_sswe.add(Flatten())\n",
    "    model_sswe.add(Dropout(0.4))\n",
    "    model_sswe.add(Dense(64, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model_sswe.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    #merged = concatenate([model_glove,model_sswe])\n",
    "    merged = Add()([model_glove.output,model_sswe.output])\n",
    "    #concatenated = concatenate([model1_out, model2_out])\n",
    "    out = Dense(4, activation='softmax', name='output_layer')(merged)\n",
    "\n",
    "    merged_model = Model([model_glove.input,model_sswe.input], out)\n",
    "    \n",
    "    \n",
    "    \n",
    "    merged_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    merged_model.summary()\n",
    "    \n",
    "    return merged_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "# %load preprocessing.py\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english')) - set(('not', 'no'))\n",
    "\n",
    "\n",
    "tags = ['<url>', '<email>', '<user>', '<hashtag>', '</hashtag>',\n",
    "        '<elongated>', '</elongated>', '<repeated>', '</repeated>']\n",
    "\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'user'],\n",
    "    annotate={'hashtag', 'elongated', 'repeated'},\n",
    "    segmenter=\"twitter\",\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,\n",
    "    unpack_contractions=True,\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    txt = text_processor.pre_process_doc(text)\n",
    "    return list(filter(lambda x: x not in tags and\n",
    "                                 x not in stopwords and\n",
    "                                 x not in punctuation, txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load load.py\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "label2emotion = {0:\"others\", 1:\"happy\", 2:\"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}\n",
    "\n",
    "emotion2label_angry = {\"others\":0, \"happy\":0, \"sad\":0, \"angry\":3}\n",
    "emotion2label_sad = {\"others\":0, \"happy\":0, \"sad\":2, \"angry\":0}\n",
    "emotion2label_happy = {\"others\":0, \"happy\":1, \"sad\":0, \"angry\":0}\n",
    "\n",
    "\n",
    "def load_data(path, training):\n",
    "    data = pd.read_csv(path, encoding='utf-8', sep='\\t')\n",
    "    text = data[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "    if not training:\n",
    "        return data['id'], text\n",
    "    else:\n",
    "        return data['id'], text, data['label']\n",
    "def load_preprocessed_data(path, training=True):\n",
    "    if not training:\n",
    "        id, text = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        return id.values.tolist(), t.values.tolist()\n",
    "    else:\n",
    "        id, text, label = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        l = label.apply(lambda x: emotion2label[x])\n",
    "        return id.values.tolist(), t.values.tolist(), l.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n",
      "Processing validation data...\n",
      "Extracting tokens...\n",
      "Found 14162 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "trainIndices, text_train, labels = load_preprocessed_data(trainDataPath)\n",
    "print(\"Processing test data...\")\n",
    "_, text_test = load_preprocessed_data(groundTruthTestData, training=False)\n",
    "print(\"Processing validation data...\")\n",
    "_, X_validation, y_validation = load_preprocessed_data(validationDataPath)\n",
    "\n",
    "print(\"Extracting tokens...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "u_trainSequences = tokenizer.texts_to_sequences(text_train)\n",
    "u_testSequences = tokenizer.texts_to_sequences(text_test)\n",
    "u_validationSequences = tokenizer.texts_to_sequences(X_validation)\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating embedding matrix...\n",
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_49 (Embedding)     (None, 64, 300)           4248900   \n",
      "_________________________________________________________________\n",
      "gru_47 (GRU)                 (None, 64, 300)           540900    \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 64, 32)            28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 32, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,822,732\n",
      "Trainable params: 4,822,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30160 samples, validate on 2755 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-14bf2235706b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         TestCallback((u_validation, labels_validation))]\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddingMatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model999.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating solution file...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2671\u001b[1;33m                                 session)\n\u001b[0m\u001b[0;32m   2672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[1;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[0;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2622\u001b[0m         \u001b[1;31m# Create callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2623\u001b[1;33m         \u001b[0mcallable_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2624\u001b[0m         \u001b[1;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2625\u001b[0m         \u001b[1;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[1;34m(self, callable_options)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \"\"\"\n\u001b[0;32m   1470\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1471\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session, callable_options)\u001b[0m\n\u001b[0;32m   1423\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[1;32m-> 1425\u001b[1;33m               session._session, options_ptr, status)\n\u001b[0m\u001b[0;32m   1426\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#glove\n",
    "print(\"Populating embedding matrix...\")\n",
    "#embeddingMatrix = getEmbeddingMatrix(wordIndex,gloveDir, 'glove.840B.300d.txt')\n",
    "u_data = pad_sequences(u_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "u_validation = pad_sequences(u_validationSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_validation = to_categorical(np.asarray(y_validation))\n",
    "\n",
    "np.random.shuffle(trainIndices)\n",
    "u_data = u_data[trainIndices]\n",
    "labels = labels[trainIndices]\n",
    "\n",
    "print(\"Building model...\")\n",
    "cbks = [ModelCheckpoint('./model1.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=2),\n",
    "        TestCallback((u_validation, labels_validation))]\n",
    "model = model10(embeddingMatrix)\n",
    "model.fit(u_data, labels, validation_data=(u_validation, labels_validation), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=cbks)\n",
    "model = load_model('./model1.h5')\n",
    "print(\"Creating solution file...\")\n",
    "create_solution_file(model, u_testSequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 64, 300)           4248900   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 4,342,600\n",
      "Trainable params: 4,342,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30160 samples, validate on 2755 samples\n",
      "Epoch 1/10\n",
      "30160/30160 [==============================] - 52s 2ms/step - loss: 0.6242 - acc: 0.7554 - val_loss: 0.3286 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32864, saving model to ./modellstmglove.h5\n",
      "True Positives per class :  [2111.  104.   94.  124.]\n",
      "False Positives per class :  [84. 81. 75. 82.]\n",
      "False Negatives per class :  [227.  38.  31.  26.]\n",
      "Class happy : Precision : 0.562, Recall : 0.732, F1 : 0.636\n",
      "Class sad : Precision : 0.556, Recall : 0.752, F1 : 0.639\n",
      "Class angry : Precision : 0.602, Recall : 0.827, F1 : 0.697\n",
      "Ignoring the Others class, Macro Precision : 0.5734, Macro Recall : 0.7704, Macro F1 : 0.6575\n",
      "Ignoring the Others class, Micro TP : 322, FP : 238, FN : 95\n",
      "Accuracy : 0.8831, Micro Precision : 0.5750, Micro Recall : 0.7722, Micro F1 : 0.6592\n",
      "Epoch 2/10\n",
      "30160/30160 [==============================] - 52s 2ms/step - loss: 0.3430 - acc: 0.8778 - val_loss: 0.3509 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.32864\n",
      "True Positives per class :  [2075.  110.   93.  132.]\n",
      "False Positives per class :  [ 69.  80.  68. 128.]\n",
      "False Negatives per class :  [263.  32.  32.  18.]\n",
      "Class happy : Precision : 0.579, Recall : 0.775, F1 : 0.663\n",
      "Class sad : Precision : 0.578, Recall : 0.744, F1 : 0.650\n",
      "Class angry : Precision : 0.508, Recall : 0.880, F1 : 0.644\n",
      "Ignoring the Others class, Macro Precision : 0.5548, Macro Recall : 0.7995, Macro F1 : 0.6550\n",
      "Ignoring the Others class, Micro TP : 335, FP : 276, FN : 82\n",
      "Accuracy : 0.8748, Micro Precision : 0.5483, Micro Recall : 0.8034, Micro F1 : 0.6518\n",
      "Epoch 3/10\n",
      "30160/30160 [==============================] - 52s 2ms/step - loss: 0.2740 - acc: 0.9029 - val_loss: 0.3859 - val_acc: 0.8566\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.32864\n",
      "True Positives per class :  [2019.  110.  102.  129.]\n",
      "False Positives per class :  [ 63. 113.  92. 127.]\n",
      "False Negatives per class :  [319.  32.  23.  21.]\n",
      "Class happy : Precision : 0.493, Recall : 0.775, F1 : 0.603\n",
      "Class sad : Precision : 0.526, Recall : 0.816, F1 : 0.639\n",
      "Class angry : Precision : 0.504, Recall : 0.860, F1 : 0.635\n",
      "Ignoring the Others class, Macro Precision : 0.5077, Macro Recall : 0.8169, Macro F1 : 0.6262\n",
      "Ignoring the Others class, Micro TP : 341, FP : 332, FN : 76\n",
      "Accuracy : 0.8566, Micro Precision : 0.5067, Micro Recall : 0.8177, Micro F1 : 0.6257\n",
      "Done training\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#glove - lstm\n",
    "'''\n",
    "print(\"Populating embedding matrix...\")\n",
    "embeddingMatrix = getEmbeddingMatrix(wordIndex,gloveDir, 'glove.840B.300d.txt')\n",
    "u_data = pad_sequences(u_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "u_validation = pad_sequences(u_validationSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_validation = to_categorical(np.asarray(y_validation))\n",
    "\n",
    "np.random.shuffle(trainIndices)\n",
    "u_data = u_data[trainIndices]\n",
    "labels = labels[trainIndices]\n",
    "'''\n",
    "print(\"Building model...\")\n",
    "cbks = [ModelCheckpoint('./modellstmglove.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=2),\n",
    "        TestCallback((u_validation, labels_validation))]\n",
    "model = modelLstm(embeddingMatrix)\n",
    "model.fit(u_data, labels, validation_data=(u_validation, labels_validation), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=cbks)\n",
    "print(\"Done training\")\n",
    "model = load_model('./modellstmglove.h5')\n",
    "#print(\"Creating solution file...\")\n",
    "#model.layers[0].get_weights()[0]\n",
    "print(len(model.layers))\n",
    "#create_solution_file(model, u_testSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4)\n",
      "(64,)\n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./modellstmglove.h5')\n",
    "print(model.layers[2].get_weights()[0].shape)\n",
    "#print(len(model.layers[1].get_weights()))\n",
    "\n",
    "glove_lstm_weights = np.zeros(shape = (64,1))\n",
    "\n",
    "x = model.layers[2].get_weights()[0]\n",
    "glove_lstm_weights = np.sum(x, axis = 1)\n",
    "print(glove_lstm_weights.shape)\n",
    "glove_lstm_weights = glove_lstm_weights.reshape(glove_lstm_weights.shape[0],1)\n",
    "print(glove_lstm_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating embedding matrix...\n",
      "Found 137052 word vectors.\n",
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 300)           4248900   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64, 300)           540900    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 64, 32)            28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,822,732\n",
      "Trainable params: 4,822,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30160 samples, validate on 2755 samples\n",
      "Epoch 1/10\n",
      "30160/30160 [==============================] - 117s 4ms/step - loss: 0.8123 - acc: 0.6838 - val_loss: 0.3857 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38570, saving model to ./modelsswe.h5\n",
      "True Positives per class :  [2084.  105.  100.  122.]\n",
      "False Positives per class :  [ 79.  72.  89. 104.]\n",
      "False Negatives per class :  [254.  37.  25.  28.]\n",
      "Class happy : Precision : 0.593, Recall : 0.739, F1 : 0.658\n",
      "Class sad : Precision : 0.529, Recall : 0.800, F1 : 0.637\n",
      "Class angry : Precision : 0.540, Recall : 0.813, F1 : 0.649\n",
      "Ignoring the Others class, Macro Precision : 0.5540, Macro Recall : 0.7843, Macro F1 : 0.6494\n",
      "Ignoring the Others class, Micro TP : 327, FP : 265, FN : 90\n",
      "Accuracy : 0.8751, Micro Precision : 0.5524, Micro Recall : 0.7842, Micro F1 : 0.6482\n",
      "Epoch 2/10\n",
      "30160/30160 [==============================] - 123s 4ms/step - loss: 0.3989 - acc: 0.8622 - val_loss: 0.3629 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38570 to 0.36293, saving model to ./modelsswe.h5\n",
      "True Positives per class :  [2078.  106.   95.  127.]\n",
      "False Positives per class :  [ 76.  85.  84. 104.]\n",
      "False Negatives per class :  [260.  36.  30.  23.]\n",
      "Class happy : Precision : 0.555, Recall : 0.746, F1 : 0.637\n",
      "Class sad : Precision : 0.531, Recall : 0.760, F1 : 0.625\n",
      "Class angry : Precision : 0.550, Recall : 0.847, F1 : 0.667\n",
      "Ignoring the Others class, Macro Precision : 0.5452, Macro Recall : 0.7844, Macro F1 : 0.6433\n",
      "Ignoring the Others class, Micro TP : 328, FP : 273, FN : 89\n",
      "Accuracy : 0.8733, Micro Precision : 0.5458, Micro Recall : 0.7866, Micro F1 : 0.6444\n",
      "Epoch 3/10\n",
      "30160/30160 [==============================] - 131s 4ms/step - loss: 0.3057 - acc: 0.8952 - val_loss: 0.4017 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36293\n",
      "True Positives per class :  [2031.  110.   96.  126.]\n",
      "False Positives per class :  [ 70. 113. 100. 109.]\n",
      "False Negatives per class :  [307.  32.  29.  24.]\n",
      "Class happy : Precision : 0.493, Recall : 0.775, F1 : 0.603\n",
      "Class sad : Precision : 0.490, Recall : 0.768, F1 : 0.598\n",
      "Class angry : Precision : 0.536, Recall : 0.840, F1 : 0.655\n",
      "Ignoring the Others class, Macro Precision : 0.5064, Macro Recall : 0.7942, Macro F1 : 0.6185\n",
      "Ignoring the Others class, Micro TP : 332, FP : 322, FN : 85\n",
      "Accuracy : 0.8577, Micro Precision : 0.5076, Micro Recall : 0.7962, Micro F1 : 0.6200\n",
      "Epoch 4/10\n",
      "30160/30160 [==============================] - 137s 5ms/step - loss: 0.2457 - acc: 0.9177 - val_loss: 0.4392 - val_acc: 0.8494\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36293\n",
      "True Positives per class :  [2005.  106.  101.  128.]\n",
      "False Positives per class :  [ 63.  99. 111. 142.]\n",
      "False Negatives per class :  [333.  36.  24.  22.]\n",
      "Class happy : Precision : 0.517, Recall : 0.746, F1 : 0.611\n",
      "Class sad : Precision : 0.476, Recall : 0.808, F1 : 0.599\n",
      "Class angry : Precision : 0.474, Recall : 0.853, F1 : 0.610\n",
      "Ignoring the Others class, Macro Precision : 0.4892, Macro Recall : 0.8026, Macro F1 : 0.6079\n",
      "Ignoring the Others class, Micro TP : 335, FP : 352, FN : 82\n",
      "Accuracy : 0.8494, Micro Precision : 0.4876, Micro Recall : 0.8034, Micro F1 : 0.6069\n",
      "Creating solution file...\n",
      "Completed. Model parameters: \n",
      "Learning rate : 0.001, LSTM Dim : 300, Dropout : 0.200, Batch_size : 200\n"
     ]
    }
   ],
   "source": [
    "#sswe\n",
    "print(\"Populating embedding matrix...\")\n",
    "embeddingMatrix = getEmbeddingMatrix(wordIndex,ssweDir, \"sswe-r.txt\")\n",
    "u_data = pad_sequences(u_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "u_validation = pad_sequences(u_validationSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_validation = to_categorical(np.asarray(y_validation))\n",
    "\n",
    "np.random.shuffle(trainIndices)\n",
    "u_data = u_data[trainIndices]\n",
    "labels = labels[trainIndices]\n",
    "\n",
    "print(\"Building model...\")\n",
    "cbks = [ModelCheckpoint('./modelsswe.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=2),\n",
    "        TestCallback((u_validation, labels_validation))]\n",
    "model = model10(embeddingMatrix)\n",
    "model.fit(u_data, labels, validation_data=(u_validation, labels_validation), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=cbks)\n",
    "model = load_model('./modelsswe.h5')\n",
    "print(\"Creating solution file...\")\n",
    "create_solution_file(model, u_testSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating embedding matrix...\n",
      "Found 137052 word vectors.\n",
      "Building model...\n",
      "WARNING:tensorflow:From C:\\Users\\vaish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 300)           4248900   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 4,342,600\n",
      "Trainable params: 4,342,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\vaish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 30160 samples, validate on 2755 samples\n",
      "Epoch 1/10\n",
      "30160/30160 [==============================] - 48s 2ms/step - loss: 0.7136 - acc: 0.7224 - val_loss: 0.3537 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35367, saving model to ./modelsswelstm.h5\n",
      "True Positives per class :  [2105.  108.   91.  116.]\n",
      "False Positives per class :  [87. 97. 75. 76.]\n",
      "False Negatives per class :  [233.  34.  34.  34.]\n",
      "Class happy : Precision : 0.527, Recall : 0.761, F1 : 0.622\n",
      "Class sad : Precision : 0.548, Recall : 0.728, F1 : 0.625\n",
      "Class angry : Precision : 0.604, Recall : 0.773, F1 : 0.678\n",
      "Ignoring the Others class, Macro Precision : 0.5597, Macro Recall : 0.7540, Macro F1 : 0.6425\n",
      "Ignoring the Others class, Micro TP : 315, FP : 248, FN : 102\n",
      "Accuracy : 0.8784, Micro Precision : 0.5595, Micro Recall : 0.7554, Micro F1 : 0.6429\n",
      "Epoch 2/10\n",
      "30160/30160 [==============================] - 46s 2ms/step - loss: 0.3377 - acc: 0.8785 - val_loss: 0.3778 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35367\n",
      "True Positives per class :  [2045.  112.   97.  127.]\n",
      "False Positives per class :  [ 66. 116.  90. 102.]\n",
      "False Negatives per class :  [293.  30.  28.  23.]\n",
      "Class happy : Precision : 0.491, Recall : 0.789, F1 : 0.605\n",
      "Class sad : Precision : 0.519, Recall : 0.776, F1 : 0.622\n",
      "Class angry : Precision : 0.555, Recall : 0.847, F1 : 0.670\n",
      "Ignoring the Others class, Macro Precision : 0.5215, Macro Recall : 0.8038, Macro F1 : 0.6326\n",
      "Ignoring the Others class, Micro TP : 336, FP : 308, FN : 81\n",
      "Accuracy : 0.8642, Micro Precision : 0.5217, Micro Recall : 0.8058, Micro F1 : 0.6334\n",
      "Epoch 3/10\n",
      "30160/30160 [==============================] - 46s 2ms/step - loss: 0.2662 - acc: 0.9060 - val_loss: 0.4357 - val_acc: 0.8436\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35367\n",
      "True Positives per class :  [1983.  112.   98.  131.]\n",
      "False Positives per class :  [ 60. 113. 104. 154.]\n",
      "False Negatives per class :  [355.  30.  27.  19.]\n",
      "Class happy : Precision : 0.498, Recall : 0.789, F1 : 0.610\n",
      "Class sad : Precision : 0.485, Recall : 0.784, F1 : 0.599\n",
      "Class angry : Precision : 0.460, Recall : 0.873, F1 : 0.602\n",
      "Ignoring the Others class, Macro Precision : 0.4809, Macro Recall : 0.8154, Macro F1 : 0.6049\n",
      "Ignoring the Others class, Micro TP : 341, FP : 371, FN : 76\n",
      "Accuracy : 0.8436, Micro Precision : 0.4789, Micro Recall : 0.8177, Micro F1 : 0.6041\n"
     ]
    }
   ],
   "source": [
    "#sswe lstm\n",
    "print(\"Populating embedding matrix...\")\n",
    "embeddingMatrix = getEmbeddingMatrix(wordIndex,ssweDir, \"sswe-r.txt\")\n",
    "u_data = pad_sequences(u_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "u_validation = pad_sequences(u_validationSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_validation = to_categorical(np.asarray(y_validation))\n",
    "\n",
    "np.random.shuffle(trainIndices)\n",
    "u_data = u_data[trainIndices]\n",
    "labels = labels[trainIndices]\n",
    "\n",
    "print(\"Building model...\")\n",
    "cbks = [ModelCheckpoint('./modelsswelstm.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=2),\n",
    "        TestCallback((u_validation, labels_validation))]\n",
    "model = modelLstm(embeddingMatrix)\n",
    "model.fit(u_data, labels, validation_data=(u_validation, labels_validation), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=cbks)\n",
    "model = load_model('./modelsswelstm.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4)\n",
      "(64,)\n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./modelsswelstm.h5')\n",
    "print(model.layers[2].get_weights()[0].shape)\n",
    "#print(len(model.layers[1].get_weights()))\n",
    "\n",
    "sswe_lstm_weights = np.zeros(shape = (64,1))\n",
    "\n",
    "x = model.layers[2].get_weights()[0]\n",
    "sswe_lstm_weights = np.sum(x, axis = 1)\n",
    "print(sswe_lstm_weights.shape)\n",
    "sswe_lstm_weights = sswe_lstm_weights.reshape(sswe_lstm_weights.shape[0],1)\n",
    "print(sswe_lstm_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1)\n"
     ]
    }
   ],
   "source": [
    "glove_sswe_combined = np.concatenate((sswe_lstm_weights, glove_lstm_weights))\n",
    "print(glove_sswe_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30160, 64)\n"
     ]
    }
   ],
   "source": [
    "print(u_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating embedding matrix...\n",
      "Populating embedding matrix...\n",
      "Building model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedding_64_input (InputLayer) (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_65_input (InputLayer) (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_64 (Embedding)        (None, 64, 300)      4248900     embedding_64_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_65 (Embedding)        (None, 64, 300)      4248900     embedding_65_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "gru_62 (GRU)                    (None, 64, 300)      540900      embedding_64[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gru_63 (GRU)                    (None, 64, 300)      540900      embedding_65[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 64, 32)       28832       gru_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, 64, 32)       28832       gru_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 32, 32)       0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_63 (MaxPooling1D) (None, 32, 32)       0           conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_62 (Flatten)            (None, 1024)         0           max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_63 (Flatten)            (None, 1024)         0           max_pooling1d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 1024)         0           flatten_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 1024)         0           flatten_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_70 (Dense)                (None, 64)           65600       dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_71 (Dense)                (None, 64)           65600       dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 64)           0           dense_70[0][0]                   \n",
      "                                                                 dense_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 4)            260         add_17[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 9,768,724\n",
      "Trainable params: 9,768,724\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 30160 samples, validate on 2755 samples\n",
      "Epoch 1/10\n",
      "30160/30160 [==============================] - 528s 17ms/step - loss: 1.2566 - acc: 0.4832 - val_loss: 0.8664 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.86636, saving model to ./modelcombined.h5\n",
      "True Positives per class :  [2338.  142.  125.  150.]\n",
      "False Positives per class :  [ 417. 2613. 2630. 2605.]\n",
      "False Negatives per class :  [0. 0. 0. 0.]\n",
      "Class happy : Precision : 0.052, Recall : 1.000, F1 : 0.098\n",
      "Class sad : Precision : 0.045, Recall : 1.000, F1 : 0.087\n",
      "Class angry : Precision : 0.054, Recall : 1.000, F1 : 0.103\n",
      "Ignoring the Others class, Macro Precision : 0.0505, Macro Recall : 1.0000, Macro F1 : 0.0961\n",
      "Ignoring the Others class, Micro TP : 417, FP : 7848, FN : 0\n",
      "Accuracy : 0.8486, Micro Precision : 0.0505, Micro Recall : 1.0000, Micro F1 : 0.0961\n",
      "Epoch 2/10\n",
      "30160/30160 [==============================] - 537s 18ms/step - loss: 1.2439 - acc: 0.4956 - val_loss: 0.8676 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.86636\n",
      "True Positives per class :  [2338.  142.  125.  150.]\n",
      "False Positives per class :  [ 417. 2613. 2630. 2605.]\n",
      "False Negatives per class :  [0. 0. 0. 0.]\n",
      "Class happy : Precision : 0.052, Recall : 1.000, F1 : 0.098\n",
      "Class sad : Precision : 0.045, Recall : 1.000, F1 : 0.087\n",
      "Class angry : Precision : 0.054, Recall : 1.000, F1 : 0.103\n",
      "Ignoring the Others class, Macro Precision : 0.0505, Macro Recall : 1.0000, Macro F1 : 0.0961\n",
      "Ignoring the Others class, Micro TP : 417, FP : 7848, FN : 0\n",
      "Accuracy : 0.8486, Micro Precision : 0.0505, Micro Recall : 1.0000, Micro F1 : 0.0961\n",
      "Epoch 3/10\n",
      "30160/30160 [==============================] - 499s 17ms/step - loss: 1.2440 - acc: 0.4956 - val_loss: 0.8669 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.86636\n",
      "True Positives per class :  [2338.  142.  125.  150.]\n",
      "False Positives per class :  [ 417. 2613. 2630. 2605.]\n",
      "False Negatives per class :  [0. 0. 0. 0.]\n",
      "Class happy : Precision : 0.052, Recall : 1.000, F1 : 0.098\n",
      "Class sad : Precision : 0.045, Recall : 1.000, F1 : 0.087\n",
      "Class angry : Precision : 0.054, Recall : 1.000, F1 : 0.103\n",
      "Ignoring the Others class, Macro Precision : 0.0505, Macro Recall : 1.0000, Macro F1 : 0.0961\n",
      "Ignoring the Others class, Micro TP : 417, FP : 7848, FN : 0\n",
      "Accuracy : 0.8486, Micro Precision : 0.0505, Micro Recall : 1.0000, Micro F1 : 0.0961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fbe7924278>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combined\n",
    "print(\"Populating embedding matrix...\")\n",
    "print(\"Populating embedding matrix...\")\n",
    "#ssweembeddingMatrix = getEmbeddingMatrix(wordIndex,ssweDir, \"sswe-r.txt\")\n",
    "#gloveembeddingMatrix = getEmbeddingMatrix(wordIndex,gloveDir, 'glove.840B.300d.txt')\n",
    "u_data = pad_sequences(u_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "u_validation = pad_sequences(u_validationSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_validation = to_categorical(np.asarray(y_validation))\n",
    "\n",
    "np.random.shuffle(trainIndices)\n",
    "u_data = u_data[trainIndices]\n",
    "labels = labels[trainIndices]\n",
    "\n",
    "print(\"Building model...\")\n",
    "cbks = [ModelCheckpoint('./modelcombined.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=2),\n",
    "        TestCallback(([np.array(u_validation),np.array(u_validation)], labels_validation))]\n",
    "model = custom_arch(gloveembeddingMatrix,ssweembeddingMatrix)\n",
    "model.fit([np.array(u_data),np.array(u_data)], np.array(labels), validation_data=([np.array(u_validation),np.array(u_validation)], np.array(labels_validation)), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=cbks)\n",
    "#model = load_model('./model99991.h5')\n",
    "#print(\"Creating solution file...\")\n",
    "#create_solution_file(model, u_testSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solution file...\n",
      "Completed. Model parameters: \n",
      "Learning rate : 0.001, LSTM Dim : 300, Dropout : 0.200, Batch_size : 200\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./modelcombined.h5')\n",
    "print(\"Creating solution file...\")\n",
    "create_solution_file2(model, u_testSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (None, 64, 300)           4248900   \n",
      "_________________________________________________________________\n",
      "gru_26 (GRU)                 (None, 64, 300)           540900    \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 64, 32)            28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 32, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,822,732\n",
      "Trainable params: 4,822,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_34 to have shape (4,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-c601ec8cb5e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         TestCallback((u_validation, labels_validation))]\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgloveembeddingMatrix\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./modelsswelstm.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating solution file...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_34 to have shape (4,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "print(\"Building model...\")\n",
    "cbks = [ModelCheckpoint('./modelsswelstm.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=2),\n",
    "        TestCallback((u_validation, labels_validation))]\n",
    "model = model10(gloveembeddingMatrix )\n",
    "model.fit(u_data, labels, validation_data=(u_validation, labels_validation), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=cbks)\n",
    "model = load_model('./modelsswelstm.h5')\n",
    "print(\"Creating solution file...\")\n",
    "create_solution_file(model, u_testSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_metrics(predictions,ground):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    \"\"\"\n",
    "    \n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    #discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "\n",
    "    truePositives = np.sum(predictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(predictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-predictions, 0, 1), axis=0)\n",
    "\n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "\n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(0, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "\n",
    "    macroPrecision /= 4\n",
    "    macroRecall /= 4\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Not Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))\n",
    "\n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()\n",
    "\n",
    "    print(\"Not Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "\n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "\n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "\n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groundTruthTestData = pd.read_csv(groundTruthTestData, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5509, 5)\n"
     ]
    }
   ],
   "source": [
    "print(groundTruthTestData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n",
      "Extracting tokens...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "trainIndices, text_train, labels = load_preprocessed_data(trainDataPath)\n",
    "print(\"Processing test data...\")\n",
    "_, text_test = load_preprocessed_data(groundTruthTestData, training=False)\n",
    "print(\"Extracting tokens...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "u_trainSequences = tokenizer.texts_to_sequences(text_train)\n",
    "u_testSequences = tokenizer.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating solution file...\n",
      "Completed. Model parameters: \n",
      "Learning rate : 0.001, LSTM Dim : 300, Dropout : 0.200, Batch_size : 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_model('./modelsswe.h5')\n",
    "print(\"Creating solution file...\")\n",
    "create_solution_file(model, u_testSequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, y_test_groundtruth = load_preprocessed_data(groundTruthTestData)\n",
    "_, _, y_test_predicted = load_preprocessed_data(solutionPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = (to_categorical(np.array(y_test_groundtruth)))\n",
    "predicted = (to_categorical(np.array(y_test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [4677.  284.  250.  298.]\n",
      "False Positives per class :  [ 832. 5225. 5259. 5211.]\n",
      "False Negatives per class :  [0. 0. 0. 0.]\n",
      "Class others : Precision : 0.849, Recall : 1.000, F1 : 0.918\n",
      "Class happy : Precision : 0.052, Recall : 1.000, F1 : 0.098\n",
      "Class sad : Precision : 0.045, Recall : 1.000, F1 : 0.087\n",
      "Class angry : Precision : 0.054, Recall : 1.000, F1 : 0.103\n",
      "Not Ignoring the Others class, Macro Precision : 0.2500, Macro Recall : 1.0000, Macro F1 : 0.4000\n",
      "Not Ignoring the Others class, Micro TP : 832, FP : 15695, FN : 0\n",
      "Accuracy : 0.8490, Micro Precision : 0.0503, Micro Recall : 1.0000, Micro F1 : 0.0959\n"
     ]
    }
   ],
   "source": [
    "get_test_metrics(predicted, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative function for getting metric\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def get_metrics_alternative(y_true, y_pred):\n",
    "         \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "         Input: shape [# of samples]\n",
    "            predictions : Model output labels eg. [1 2 3 0 1 2...]\n",
    "            ground : Ground truth labels eg. [1 2 3 0 1 2...]]\n",
    "        \"\"\"\n",
    "         p_r_fbeta = (precision_recall_fscore_support(y_true, y_pred, average='macro'))\n",
    "         acc = accuracy_score(y_true, y_pred)\n",
    "         print(\"Macro Precision = \", p_r_fbeta[0])\n",
    "         print(\"Macro Recall = \", p_r_fbeta[1])\n",
    "         print(\"Macro F1 = \", p_r_fbeta[2])\n",
    "         print(\"Accuracy = \", acc)\n",
    "\n",
    "         p_r_fbeta = (precision_recall_fscore_support(y_true, y_pred, average='micro'))\n",
    "         print(\"Micro Precision = \", p_r_fbeta[0])\n",
    "         print(\"Micro Recall = \", p_r_fbeta[1])\n",
    "         print(\"Micro F1 = \", p_r_fbeta[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision =  0.21224360137956072\n",
      "Macro Recall =  0.25\n",
      "Macro F1 =  0.22957981543294717\n",
      "Accuracy =  0.8489744055182429\n",
      "Micro Precision =  0.8489744055182429\n",
      "Micro Recall =  0.8489744055182429\n",
      "Micro F1 =  0.8489744055182429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaish\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "get_metrics_alternative(y_test_groundtruth,y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
