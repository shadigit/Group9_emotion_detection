{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sys import path as pylib #im naming it as pylib so that we won't get confused between os.path and sys.path \n",
    "import os\n",
    "pylib += [os.path.abspath(r'/home/Jay/Notebooks/Group9_emotion_detection/torchMoji')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../deepmoji/data/train.txt', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "label2emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
    "emotion2label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}\n",
    "\n",
    "emoticons_additional = {\n",
    "    '(^・^)': '<happy>', ':‑c': '<sad>', '=‑d': '<happy>', \":'‑)\": '<happy>', ':‑d': '<laugh>',\n",
    "    ':‑(': '<sad>', ';‑)': '<happy>', ':‑)': '<happy>', ':\\\\/': '<sad>', 'd=<': '<annoyed>',\n",
    "    ':‑/': '<annoyed>', ';‑]': '<happy>', '(^�^)': '<happy>', 'angru': 'angry', \"d‑':\":\n",
    "        '<annoyed>', \":'‑(\": '<sad>', \":‑[\": '<annoyed>', '(�?�)': '<happy>', 'x‑d': '<laugh>',\n",
    "}\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "               'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "              'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\",\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons, emoticons_additional]\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(text_processor.pre_process_doc(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=[]\n",
    "x2=[]\n",
    "x3=[]\n",
    "y=[]\n",
    "labels={}\n",
    "i=0\n",
    "for ind, row in data.iterrows():\n",
    "    if row['label'] not in labels:\n",
    "        labels[row['label']]=i\n",
    "        i+=1\n",
    "    y.append(labels[row['label']])\n",
    "    x1.append(tokenize(row['turn1']))\n",
    "    x2.append(tokenize(row['turn2']))\n",
    "    x3.append(tokenize(row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../../deepmoji/data/test.txt', sep = '\\t')\n",
    "test_x1=[]\n",
    "test_x2=[]\n",
    "test_x3=[]\n",
    "test_y=[]\n",
    "test_labels={}\n",
    "i=0\n",
    "for ind, row in test_data.iterrows():\n",
    "    if row['label'] not in test_labels:\n",
    "        test_labels[row['label']]=i\n",
    "        i+=1\n",
    "    test_y.append(test_labels[row['label']])\n",
    "    test_x1.append(tokenize(row['turn1']))\n",
    "    test_x2.append(tokenize(row['turn2']))\n",
    "    test_x3.append(tokenize(row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import json\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_feature_encoding\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "def encode_deepmoji(x):\n",
    "    maxlen = 30\n",
    "    batch_size = 32\n",
    "\n",
    "    print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "    with open(VOCAB_PATH, 'r') as f:\n",
    "        vocabulary = json.load(f)\n",
    "    st = SentenceTokenizer(vocabulary, maxlen)\n",
    "    tokenized, _, _ = st.tokenize_sentences(x)\n",
    "    print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "    model = torchmoji_feature_encoding(PRETRAINED_PATH)\n",
    "    print(model)\n",
    "    print('Encoding texts..')\n",
    "    encoding = np.zeros((len(x), 2304))\n",
    "    for i in range(0, len(x), 300):    \n",
    "        encoding[i:i+300] = model(tokenized[i:i+300])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing using dictionary from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/vocabulary.json\n",
      "Loading model from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/pytorch_model.bin.\n",
      "Loading weights for embed.weight\n",
      "Loading weights for lstm_0.weight_ih_l0\n",
      "Loading weights for lstm_0.weight_hh_l0\n",
      "Loading weights for lstm_0.bias_ih_l0\n",
      "Loading weights for lstm_0.bias_hh_l0\n",
      "Loading weights for lstm_0.weight_ih_l0_reverse\n",
      "Loading weights for lstm_0.weight_hh_l0_reverse\n",
      "Loading weights for lstm_0.bias_ih_l0_reverse\n",
      "Loading weights for lstm_0.bias_hh_l0_reverse\n",
      "Loading weights for lstm_1.weight_ih_l0\n",
      "Loading weights for lstm_1.weight_hh_l0\n",
      "Loading weights for lstm_1.bias_ih_l0\n",
      "Loading weights for lstm_1.bias_hh_l0\n",
      "Loading weights for lstm_1.weight_ih_l0_reverse\n",
      "Loading weights for lstm_1.weight_hh_l0_reverse\n",
      "Loading weights for lstm_1.bias_ih_l0_reverse\n",
      "Loading weights for lstm_1.bias_hh_l0_reverse\n",
      "Loading weights for attention_layer.attention_vector\n",
      "Ignoring weights for output_layer.0.weight\n",
      "Ignoring weights for output_layer.0.bias\n",
      "TorchMoji(\n",
      "  (embed): Embedding(50000, 256)\n",
      "  (embed_dropout): Dropout2d(p=0)\n",
      "  (lstm_0): LSTMHardSigmoid(256, 512, batch_first=True, bidirectional=True)\n",
      "  (lstm_1): LSTMHardSigmoid(1024, 512, batch_first=True, bidirectional=True)\n",
      "  (attention_layer): Attention(2304, return attention=False)\n",
      ")\n",
      "Encoding texts..\n"
     ]
    }
   ],
   "source": [
    "x1_vec = encode_deepmoji(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing using dictionary from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/vocabulary.json\n",
      "Loading model from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/pytorch_model.bin.\n",
      "Loading weights for embed.weight\n",
      "Loading weights for lstm_0.weight_ih_l0\n",
      "Loading weights for lstm_0.weight_hh_l0\n",
      "Loading weights for lstm_0.bias_ih_l0\n",
      "Loading weights for lstm_0.bias_hh_l0\n",
      "Loading weights for lstm_0.weight_ih_l0_reverse\n",
      "Loading weights for lstm_0.weight_hh_l0_reverse\n",
      "Loading weights for lstm_0.bias_ih_l0_reverse\n",
      "Loading weights for lstm_0.bias_hh_l0_reverse\n",
      "Loading weights for lstm_1.weight_ih_l0\n",
      "Loading weights for lstm_1.weight_hh_l0\n",
      "Loading weights for lstm_1.bias_ih_l0\n",
      "Loading weights for lstm_1.bias_hh_l0\n",
      "Loading weights for lstm_1.weight_ih_l0_reverse\n",
      "Loading weights for lstm_1.weight_hh_l0_reverse\n",
      "Loading weights for lstm_1.bias_ih_l0_reverse\n",
      "Loading weights for lstm_1.bias_hh_l0_reverse\n",
      "Loading weights for attention_layer.attention_vector\n",
      "Ignoring weights for output_layer.0.weight\n",
      "Ignoring weights for output_layer.0.bias\n",
      "TorchMoji(\n",
      "  (embed): Embedding(50000, 256)\n",
      "  (embed_dropout): Dropout2d(p=0)\n",
      "  (lstm_0): LSTMHardSigmoid(256, 512, batch_first=True, bidirectional=True)\n",
      "  (lstm_1): LSTMHardSigmoid(1024, 512, batch_first=True, bidirectional=True)\n",
      "  (attention_layer): Attention(2304, return attention=False)\n",
      ")\n",
      "Encoding texts..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-59e7a04b1b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx2_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_deepmoji\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-85b6498fc1a9>\u001b[0m in \u001b[0;36mencode_deepmoji\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2304\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev32/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Notebooks/Group9_emotion_detection/torchMoji/torchmoji/model_def.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seqs)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# skip-connection from embedding to output eases gradient-flow and allows access to lower-level features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;31m# ordering of the way the merge is done is important for consistency with the pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mlstm_0_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mlstm_1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_0_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev32/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Notebooks/Group9_emotion_detection/torchMoji/torchmoji/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         )\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Notebooks/Group9_emotion_detection/torchMoji/torchmoji/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Notebooks/Group9_emotion_detection/torchMoji/torchmoji/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Notebooks/Group9_emotion_detection/torchMoji/torchmoji/lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Notebooks/Group9_emotion_detection/torchMoji/torchmoji/lstm.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \"\"\"\n\u001b[1;32m    333\u001b[0m     \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mgates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mingate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x2_vec = encode_deepmoji(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_vec = encode_deepmoji(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing using dictionary from /home/Jay/Notebooks/deepmoji/torchMoji/model/vocabulary.json\n",
      "Loading model from /home/Jay/Notebooks/deepmoji/torchMoji/model/pytorch_model.bin.\n",
      "Loading weights for embed.weight\n",
      "Loading weights for lstm_0.weight_ih_l0\n",
      "Loading weights for lstm_0.weight_hh_l0\n",
      "Loading weights for lstm_0.bias_ih_l0\n",
      "Loading weights for lstm_0.bias_hh_l0\n",
      "Loading weights for lstm_0.weight_ih_l0_reverse\n",
      "Loading weights for lstm_0.weight_hh_l0_reverse\n",
      "Loading weights for lstm_0.bias_ih_l0_reverse\n",
      "Loading weights for lstm_0.bias_hh_l0_reverse\n",
      "Loading weights for lstm_1.weight_ih_l0\n",
      "Loading weights for lstm_1.weight_hh_l0\n",
      "Loading weights for lstm_1.bias_ih_l0\n",
      "Loading weights for lstm_1.bias_hh_l0\n",
      "Loading weights for lstm_1.weight_ih_l0_reverse\n",
      "Loading weights for lstm_1.weight_hh_l0_reverse\n",
      "Loading weights for lstm_1.bias_ih_l0_reverse\n",
      "Loading weights for lstm_1.bias_hh_l0_reverse\n",
      "Loading weights for attention_layer.attention_vector\n",
      "Ignoring weights for output_layer.0.weight\n",
      "Ignoring weights for output_layer.0.bias\n",
      "TorchMoji(\n",
      "  (embed): Embedding(50000, 256)\n",
      "  (embed_dropout): Dropout2d(p=0)\n",
      "  (lstm_0): LSTMHardSigmoid(256, 512, batch_first=True, bidirectional=True)\n",
      "  (lstm_1): LSTMHardSigmoid(1024, 512, batch_first=True, bidirectional=True)\n",
      "  (attention_layer): Attention(2304, return attention=False)\n",
      ")\n",
      "Encoding texts..\n"
     ]
    }
   ],
   "source": [
    "np.savez('deepmoji_train_x1.npy', x1_vec)\n",
    "np.savez('deepmoji_train_x2.npy', x2_vec)\n",
    "np.savez('deepmoji_train_x3.npy', x3_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding(i):\n",
    "    print('First 5 dimensions for sentence: {}'.format(x1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.load('../../dm_emb/deepmoji_train_x1.npy')\n",
    "x2 = np.load('../../dm_emb/deepmoji_train_x2.npy')\n",
    "x3 = np.load('../../dm_emb/deepmoji_train_x3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepmoji_train_all = np.c_[x1,x2,x3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing using dictionary from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/vocabulary.json\n",
      "Loading model from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/pytorch_model.bin.\n",
      "Loading weights for embed.weight\n",
      "Loading weights for lstm_0.weight_ih_l0\n",
      "Loading weights for lstm_0.weight_hh_l0\n",
      "Loading weights for lstm_0.bias_ih_l0\n",
      "Loading weights for lstm_0.bias_hh_l0\n",
      "Loading weights for lstm_0.weight_ih_l0_reverse\n",
      "Loading weights for lstm_0.weight_hh_l0_reverse\n",
      "Loading weights for lstm_0.bias_ih_l0_reverse\n",
      "Loading weights for lstm_0.bias_hh_l0_reverse\n",
      "Loading weights for lstm_1.weight_ih_l0\n",
      "Loading weights for lstm_1.weight_hh_l0\n",
      "Loading weights for lstm_1.bias_ih_l0\n",
      "Loading weights for lstm_1.bias_hh_l0\n",
      "Loading weights for lstm_1.weight_ih_l0_reverse\n",
      "Loading weights for lstm_1.weight_hh_l0_reverse\n",
      "Loading weights for lstm_1.bias_ih_l0_reverse\n",
      "Loading weights for lstm_1.bias_hh_l0_reverse\n",
      "Loading weights for attention_layer.attention_vector\n",
      "Ignoring weights for output_layer.0.weight\n",
      "Ignoring weights for output_layer.0.bias\n",
      "TorchMoji(\n",
      "  (embed): Embedding(50000, 256)\n",
      "  (embed_dropout): Dropout2d(p=0)\n",
      "  (lstm_0): LSTMHardSigmoid(256, 512, batch_first=True, bidirectional=True)\n",
      "  (lstm_1): LSTMHardSigmoid(1024, 512, batch_first=True, bidirectional=True)\n",
      "  (attention_layer): Attention(2304, return attention=False)\n",
      ")\n",
      "Encoding texts..\n",
      "Tokenizing using dictionary from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/vocabulary.json\n",
      "Loading model from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/pytorch_model.bin.\n",
      "Loading weights for embed.weight\n",
      "Loading weights for lstm_0.weight_ih_l0\n",
      "Loading weights for lstm_0.weight_hh_l0\n",
      "Loading weights for lstm_0.bias_ih_l0\n",
      "Loading weights for lstm_0.bias_hh_l0\n",
      "Loading weights for lstm_0.weight_ih_l0_reverse\n",
      "Loading weights for lstm_0.weight_hh_l0_reverse\n",
      "Loading weights for lstm_0.bias_ih_l0_reverse\n",
      "Loading weights for lstm_0.bias_hh_l0_reverse\n",
      "Loading weights for lstm_1.weight_ih_l0\n",
      "Loading weights for lstm_1.weight_hh_l0\n",
      "Loading weights for lstm_1.bias_ih_l0\n",
      "Loading weights for lstm_1.bias_hh_l0\n",
      "Loading weights for lstm_1.weight_ih_l0_reverse\n",
      "Loading weights for lstm_1.weight_hh_l0_reverse\n",
      "Loading weights for lstm_1.bias_ih_l0_reverse\n",
      "Loading weights for lstm_1.bias_hh_l0_reverse\n",
      "Loading weights for attention_layer.attention_vector\n",
      "Ignoring weights for output_layer.0.weight\n",
      "Ignoring weights for output_layer.0.bias\n",
      "TorchMoji(\n",
      "  (embed): Embedding(50000, 256)\n",
      "  (embed_dropout): Dropout2d(p=0)\n",
      "  (lstm_0): LSTMHardSigmoid(256, 512, batch_first=True, bidirectional=True)\n",
      "  (lstm_1): LSTMHardSigmoid(1024, 512, batch_first=True, bidirectional=True)\n",
      "  (attention_layer): Attention(2304, return attention=False)\n",
      ")\n",
      "Encoding texts..\n",
      "Tokenizing using dictionary from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/vocabulary.json\n",
      "Loading model from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/pytorch_model.bin.\n",
      "Loading weights for embed.weight\n",
      "Loading weights for lstm_0.weight_ih_l0\n",
      "Loading weights for lstm_0.weight_hh_l0\n",
      "Loading weights for lstm_0.bias_ih_l0\n",
      "Loading weights for lstm_0.bias_hh_l0\n",
      "Loading weights for lstm_0.weight_ih_l0_reverse\n",
      "Loading weights for lstm_0.weight_hh_l0_reverse\n",
      "Loading weights for lstm_0.bias_ih_l0_reverse\n",
      "Loading weights for lstm_0.bias_hh_l0_reverse\n",
      "Loading weights for lstm_1.weight_ih_l0\n",
      "Loading weights for lstm_1.weight_hh_l0\n",
      "Loading weights for lstm_1.bias_ih_l0\n",
      "Loading weights for lstm_1.bias_hh_l0\n",
      "Loading weights for lstm_1.weight_ih_l0_reverse\n",
      "Loading weights for lstm_1.weight_hh_l0_reverse\n",
      "Loading weights for lstm_1.bias_ih_l0_reverse\n",
      "Loading weights for lstm_1.bias_hh_l0_reverse\n",
      "Loading weights for attention_layer.attention_vector\n",
      "Ignoring weights for output_layer.0.weight\n",
      "Ignoring weights for output_layer.0.bias\n",
      "TorchMoji(\n",
      "  (embed): Embedding(50000, 256)\n",
      "  (embed_dropout): Dropout2d(p=0)\n",
      "  (lstm_0): LSTMHardSigmoid(256, 512, batch_first=True, bidirectional=True)\n",
      "  (lstm_1): LSTMHardSigmoid(1024, 512, batch_first=True, bidirectional=True)\n",
      "  (attention_layer): Attention(2304, return attention=False)\n",
      ")\n",
      "Encoding texts..\n"
     ]
    }
   ],
   "source": [
    "test_x1_vec = encode_deepmoji(test_x1)\n",
    "test_x2_vec = encode_deepmoji(test_x2)\n",
    "test_x3_vec = encode_deepmoji(test_x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepmoji_test = np.c_[np.array(test_x1_vec),np.array(test_x2_vec),np.array(test_x3_vec)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories, category_counts = np.unique(y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(categories,category_counts,tick_label=list(emotion2label.keys()),align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14948,  5506,  5463,  4243])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_others=np.where(y==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.r_[np.random.choice(idx_others,6000),np.where(y==1)[0],np.where(y==2)[0],np.where(y==3)[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21212,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepmoji_train = deepmoji_train_all[idx]\n",
    "y_train = y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories_train, category_counts_train = np.unique(y_train,return_counts=True)\n",
    "plt.bar(categories_train,category_counts_train,tick_label=list(emotion2label.keys()),align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_t, category_counts_t = np.unique(test_y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEDxJREFUeJzt3X+MZWV9x/H3BxYVxbL8mFC6SzoUNzVgU4EtQsSGQgUUW2gKirF1McQNFVtMTVpsrbQKCZSkWJJqg7IB0RYpaiFAxQ1Ctbb82OXnLhQZAQsEZWWBFlEQ+PaP+yy5rLPOzO7M3F2f9yuZzHO+5zn3PufZe+/nnnPPnU1VIUnqz3ajHoAkaTQMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnFox6AD/L7rvvXuPj46MehiRtU1avXv2Dqhqbqt9WHQDj4+OsWrVq1MOQpG1Kku9Op5+ngCSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVNb9TeBt9T46VePeggj9eDZx4x6CJK2Yh4BSFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnph0ASbZPcluSq9ry3kluSjKR5ItJXtHqr2zLE239+NBtfKTV701y1GzvjCRp+mZyBHAacM/Q8jnAeVX1OuAJ4ORWPxl4otXPa/1Isi9wIrAfcDTwqSTbb9nwJUmba1oBkGQxcAzw2bYc4HDg8tblYuC41j62LdPWH9H6HwtcWlXPVtUDwARw0GzshCRp5qZ7BPBJ4M+AF9vybsCTVfV8W34YWNTai4CHANr6p1r/l+qTbCNJmmdTBkCSdwCPVdXqeRgPSZYnWZVk1bp16+bjLiWpS9M5Angz8LtJHgQuZXDq5++BhUkWtD6LgUda+xFgL4C2fmfg8eH6JNu8pKouqKqlVbV0bGxsxjskSZqeKQOgqj5SVYurapzBh7hfr6r3ANcDx7duy4ArWvvKtkxb//WqqlY/sV0ltDewBLh51vZEkjQjC6buskl/Dlya5EzgNuDCVr8QuCTJBLCeQWhQVWuTXAbcDTwPnFpVL2zB/UuStsCMAqCqbgBuaO37meQqnqr6MXDCJrY/CzhrpoOUJM0+vwksSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1asoASPKqJDcnuSPJ2iR/0+p7J7kpyUSSLyZ5Rau/si1PtPXjQ7f1kVa/N8lRc7VTkqSpTecI4Fng8Kr6deCNwNFJDgbOAc6rqtcBTwAnt/4nA0+0+nmtH0n2BU4E9gOOBj6VZPvZ3BlJ0vRNGQA18HRb3KH9FHA4cHmrXwwc19rHtmXa+iOSpNUvrapnq+oBYAI4aFb2QpI0Y9P6DCDJ9kluBx4DVgLfAZ6squdbl4eBRa29CHgIoK1/CthtuD7JNpKkeTatAKiqF6rqjcBiBu/aXz9XA0qyPMmqJKvWrVs3V3cjSd2b0VVAVfUkcD1wCLAwyYK2ajHwSGs/AuwF0NbvDDw+XJ9km+H7uKCqllbV0rGxsZkMT5I0A9O5CmgsycLW3hF4K3APgyA4vnVbBlzR2le2Zdr6r1dVtfqJ7SqhvYElwM2ztSOSpJlZMHUX9gQublfsbAdcVlVXJbkbuDTJmcBtwIWt/4XAJUkmgPUMrvyhqtYmuQy4G3geOLWqXpjd3ZEkTdeUAVBVdwL7T1K/n0mu4qmqHwMnbOK2zgLOmvkwJUmzzW8CS1KnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTUwZAkr2SXJ/k7iRrk5zW6rsmWZnkvvZ7l1ZPkvOTTCS5M8kBQ7e1rPW/L8myudstSdJUpnME8Dzw4araFzgYODXJvsDpwHVVtQS4ri0DvA1Y0n6WA5+GQWAAZwBvAg4CztgQGpKk+TdlAFTVo1V1a2v/H3APsAg4Fri4dbsYOK61jwU+VwM3AguT7AkcBaysqvVV9QSwEjh6VvdGkjRtM/oMIMk4sD9wE7BHVT3aVn0P2KO1FwEPDW32cKttqr7xfSxPsirJqnXr1s1keJKkGZh2ACTZCfgS8KGq+t/hdVVVQM3GgKrqgqpaWlVLx8bGZuMmJUmTmFYAJNmBwYv/F6rqy638/XZqh/b7sVZ/BNhraPPFrbapuiRpBKZzFVCAC4F7qurvhlZdCWy4kmcZcMVQ/b3taqCDgafaqaJrgSOT7NI+/D2y1SRJI7BgGn3eDPwhcFeS21vtL4CzgcuSnAx8F3hnW3cN8HZgAngGeB9AVa1P8gngltbv41W1flb2QpI0Y1MGQFX9B5BNrD5ikv4FnLqJ21oBrJjJACVJc8NvAktSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ2aMgCSrEjyWJI1Q7Vdk6xMcl/7vUurJ8n5SSaS3JnkgKFtlrX+9yVZNje7I0marukcAVwEHL1R7XTguqpaAlzXlgHeBixpP8uBT8MgMIAzgDcBBwFnbAgNSdJoTBkAVfUNYP1G5WOBi1v7YuC4ofrnauBGYGGSPYGjgJVVtb6qngBW8tOhIkmaR5v7GcAeVfVoa38P2KO1FwEPDfV7uNU2VZckjcgWfwhcVQXULIwFgCTLk6xKsmrdunWzdbOSpI1sbgB8v53aof1+rNUfAfYa6re41TZV/ylVdUFVLa2qpWNjY5s5PEnSVDY3AK4ENlzJswy4Yqj+3nY10MHAU+1U0bXAkUl2aR/+HtlqkqQRWTBVhyT/DBwG7J7kYQZX85wNXJbkZOC7wDtb92uAtwMTwDPA+wCqan2STwC3tH4fr6qNP1iWJM2jKQOgqt69iVVHTNK3gFM3cTsrgBUzGp0kac74TWBJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdWjDqAWjrNX761aMewkg9ePYxW7S987dl86e55xGAJHXKIwBJWyWPoOb+CMojAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktSpeQ+AJEcnuTfJRJLT5/v+JUkD8xoASbYH/gF4G7Av8O4k+87nGCRJA/N9BHAQMFFV91fVc8ClwLHzPAZJEvMfAIuAh4aWH241SdI82+r+FESS5cDytvh0kntHOZ4ttDvwg1Hdec4Z1T3PGudvyzh/W2Zbnr9fnk6n+Q6AR4C9hpYXt9pLquoC4IL5HNRcSbKqqpaOehzbKudvyzh/W6aH+ZvvU0C3AEuS7J3kFcCJwJXzPAZJEvN8BFBVzyf5IHAtsD2woqrWzucYJEkD8/4ZQFVdA1wz3/c7Ij8Xp7JGyPnbMs7flvm5n79U1ajHIEkaAf8UhCR1ygDQnEoynmTNqMehgd7/PZIsTPKBoeXDklw1yjGNkgEwIkm2uu9gSB1YCHxgyl7TtK0/jw2AaUryr0lWJ1nbvqxGkqeTnJXkjiQ3Jtmj1fdpy3clOTPJ061+WJJvJrkSuDvJx5N8aOg+zkpy2kh2cG5tn+Qzbe6+lmTHJO9Pckubuy8leTVAkouS/GOSVUm+neQdrX5SkiuS3JDkviRntHovc/gySV6T5Oo2f2uSvCvJx9qcrklyQZK0vge2fncAp4546PMqyZ+2+VjTHidnA/skuT3Jua3bTkkuT/LfSb6w0bz9e3veX5tkz1a/Icknk6wCTktyQrv9O5J8YzR7upmqyp9p/AC7tt87AmuA3YACfqfV/xb4aGtfBby7tU8Bnm7tw4AfAnu35XHg1tbeDvgOsNuo93WW520ceB54Y1u+DPiD4f0EzgT+uLUvAr7a5mMJgz8X8irgJODRNu8b/g2W9jCHm5jX3wc+M7S884bHaFu+ZOixeSfwm619LrBm1OOfpzk6ELgLeA2wE7AW2H94/9tz8ikGX0rdDvgv4FBgB+A/gbHW710MLlsHuAH41NBt3AUsau2Fo97vmfx4BDB9f9LeQd3I4NvMS4DnGLzYA6xm8GIEcAjwL639Txvdzs1V9QBAVT0IPJ5kf+BI4LaqenyudmCEHqiq21t7wzy9oR0N3QW8B9hvqP9lVfViVd0H3A+8vtVXVtXjVfUj4MvAoR3N4cbuAt6a5Jwkb6mqp4DfSnJTm9PDgf2SLGTworThnekloxrwCBwKfKWqflhVTzN4zLxlkn43V9XDVfUicDuDx+evAm8AVia5Hfgog5DY4ItD7W8BFyV5P4PvN20ztunzV/MlyWHAbwOHVNUzSW5g8K70J9ViH3iB6c3nDzda/iyDd7e/CKyYjfFuhZ4dar/A4B38RcBxVXVHkpMYvBPbYONrk2uKeg9z+DJV9e0kBwBvB85Mch2D0ztLq+qhJH/N4DGqqW38+FwABFhbVYdsYpuXnsdVdUqSNwHHAKuTHLitvAnxCGB6dgaeaC/+rwcOnqL/jQwO0WHw5y5+lq8ARwO/weAb0r14LfBokh0YHAEMOyHJdkn2AX4F2PAHAd+aZNckOwLHMXjnBR3OYZJfAp6pqs8zOK1zQFv1gyQ7AccDVNWTwJNJDm3rN57rn2ffBI5L8uokrwF+j8Fj5rXT2PZeYCzJIQBJdkiy32Qdk+xTVTdV1ceAdbz8751t1TwCmJ6vAqckuYfBA+PGKfp/CPh8kr9s2z61qY5V9VyS64Enq+qF2RrwNuCvgJsYPGFu4uVPyv8BbgZ+ATilqn7cPpe7GfgSg0Pxz1fVKuh2Dn8NODfJi8BPgD9iEIprgO8x+LtbG7wPWJGkgK/N90BHpapuTXIRg8cNwGeranWSb2VwKey/AVdvYtvnkhwPnJ9kZwavlZ9k8DnCxs5NsoTBUcN1wB2zvCtzxm8Cz4F2RcuPqqqSnMjgA+FJ/+ObJNsBtwIntHPeXWtP2Kuq6vKN6icxOL3xwUm2cQ6lzeApoLlxIHB7kjsZXHP84ck6ZfDfYU4A1/nCtXmcQ2nzeQQgSZ3yCECSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR16v8BCgiwj89qWBQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(categories_t,category_counts_t,tick_label=list(emotion2label.keys()),align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4677,  284,  298,  250])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def print_metrics(mc_prediction):\n",
    "    print(metrics.classification_report(test_y, mc_prediction, target_names=list(emotion2label.keys())))\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(test_y, mc_prediction))\n",
    "    print(\"Precision:\", metrics.precision_score(test_y, mc_prediction, average='macro'))\n",
    "    print(\"Recall:\", metrics.recall_score(test_y, mc_prediction, average='macro'))\n",
    "    print(\"F1 score: \", metrics.f1_score(test_y, mc_prediction, average='macro'))\n",
    "    #print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(mc_Y_test, mc_prediction))\n",
    "    plot_confusion_matrix(metrics.confusion_matrix(test_y, mc_prediction), list(emotion2label.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(random_state=0).fit(deepmoji_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(LR.predict(deepmoji_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_nb = Pipeline([('scaler', MinMaxScaler()), ('clf', MultinomialNB())])\n",
    "\n",
    "mc_nb.fit(deepmoji_train, y_train)\n",
    "print_metrics(mc_nb.predict(deepmoji_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - One vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_svm_ovo = svm.SVC(decision_function_shape='ovo', kernel='linear', C=100)\n",
    "\n",
    "mc_svm_ovo.fit(deepmoji_train, y_train)\n",
    "print_metrics(mc_svm_ovo.predict(deepmoji_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM One vs Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_svm_ovr = svm.SVC(decision_function_shape='ovr', kernel='linear', C=100)\n",
    "mc_svm_ovr.fit(deepmoji_train, y_train)\n",
    "print_merics(mc_svm_ovr.predict(deepmoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
