{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from load import load_preprocessed_data, label2emotion\n",
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import argparse\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Concatenate, Reshape, GRU, Bidirectional, Dropout, Conv1D, Flatten, MaxPool1D, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(7)\n",
    "\n",
    "\n",
    "#Keras Callback to compute F1 score for each epoch\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        r = self.model.predict(x)\n",
    "        getMetrics(r, y)\n",
    "\n",
    "\n",
    "validationDataPath = \"\"\n",
    "trainDataPath = \"\"\n",
    "testDataPath = \"\"\n",
    "solutionPath = \"\"\n",
    "gloveDir = \"\"\n",
    "\n",
    "trainDataPath = '../data/starterkit/train.txt'\n",
    "testDataPath = '../data/starterkit/devwithoutlabels.txt'\n",
    "validationDataPath = '../data/devsetwithlabels.txt'\n",
    "solutionPath = 'test.txt'\n",
    "gloveDir = '../data/'\n",
    "\n",
    "NUM_FOLDS = None\n",
    "NUM_CLASSES = None\n",
    "MAX_NB_WORDS = None\n",
    "MAX_SEQUENCE_LENGTH = None\n",
    "EMBEDDING_DIM = None\n",
    "BATCH_SIZE = None\n",
    "LSTM_DIM = None\n",
    "DROPOUT = None\n",
    "LEARNING_RATE = None\n",
    "NUM_EPOCHS = None\n",
    "\n",
    "\n",
    "NUM_CLASSES = 4 # for happy\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 64\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 200\n",
    "LSTM_DIM = 300\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "DROPOUT = 0.2\n",
    "\n",
    "#load the configuration data\n",
    "def load_config(configPath):\n",
    "    with open(configPath) as configfile:\n",
    "        config = json.load(configfile)\n",
    "\n",
    "    global validationDataPath, trainDataPath, testDataPath, solutionPath, gloveDir\n",
    "    global NUM_FOLDS, NUM_CLASSES, MAX_NB_WORDS, EMBEDDING_DIM\n",
    "    global BATCH_SIZE, LSTM_DIM, DROPOUT, NUM_EPOCHS, LEARNING_RATE, MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    validationDataPath = config[\"validation_data_path\"]\n",
    "    trainDataPath = config[\"train_data_path\"]\n",
    "    testDataPath = config[\"test_data_path\"]\n",
    "    solutionPath = config[\"solution_path\"]\n",
    "    gloveDir = config[\"glove_dir\"]\n",
    "\n",
    "    NUM_FOLDS = config[\"num_folds\"]\n",
    "    NUM_CLASSES = config[\"num_classes\"]\n",
    "    MAX_NB_WORDS = config[\"max_nb_words\"]\n",
    "    MAX_SEQUENCE_LENGTH = config[\"max_sequence_length\"]\n",
    "    EMBEDDING_DIM = config[\"embedding_dim\"]\n",
    "    BATCH_SIZE = config[\"batch_size\"]\n",
    "    LSTM_DIM = config[\"lstm_dim\"]\n",
    "    DROPOUT = config[\"dropout\"]\n",
    "    LEARNING_RATE = config[\"learning_rate\"]\n",
    "    NUM_EPOCHS = config[\"num_epochs\"]\n",
    "\n",
    "\n",
    "#generate the test.txt file to be submitted\n",
    "def create_solution_file(model,u_testSequences):\n",
    "    u_testData = pad_sequences(u_testSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    predictions = model.predict(u_testData, batch_size=BATCH_SIZE)\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "\n",
    "    with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "        fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')\n",
    "        with io.open(testDataPath, encoding=\"utf8\") as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "                fout.write(label2emotion[predictions[lineNum]] + '\\n')\n",
    "    print(\"Completed. Model parameters: \")\n",
    "    print(\"Learning rate : %.3f, LSTM Dim : %d, Dropout : %.3f, Batch_size : %d\"\n",
    "          % (LEARNING_RATE, LSTM_DIM, DROPOUT, BATCH_SIZE))\n",
    "    return\n",
    "\n",
    "\n",
    "#Create the embedding matrix\n",
    "def getEmbeddingMatrix(wordIndex):\n",
    "    embeddingsIndex = {}\n",
    "    with io.open(os.path.join(gloveDir, 'glove.840B.300d.txt'), encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embeddingVector = np.array([float(val) for val in values[1:]])\n",
    "            embeddingsIndex[word] = embeddingVector\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddingsIndex))\n",
    "\n",
    "    embeddingMatrix = np.zeros((len(wordIndex) + 1, EMBEDDING_DIM))\n",
    "    for word, i in wordIndex.items():\n",
    "        embeddingVector = embeddingsIndex.get(word)\n",
    "        if embeddingVector is not None:\n",
    "            embeddingMatrix[i] = embeddingVector\n",
    "        else:\n",
    "            oov = [np.random.normal(size = EMBEDDING_DIM)]\n",
    "            oov /= np.linalg.norm(oov)\n",
    "            embeddingMatrix[i] = oov\n",
    "\n",
    "    return embeddingMatrix\n",
    "\n",
    "\n",
    "#Compute the micro F1 score, the average accuracy, the micro precision and the micro recall\n",
    "def getMetrics(predictions, ground):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification\n",
    "    \"\"\"\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "\n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "\n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "\n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "\n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))\n",
    "\n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()\n",
    "\n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "\n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "\n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions==ground)\n",
    "\n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (accuracy, microPrecision, microRecall, microF1))\n",
    "\n",
    "    return accuracy, microPrecision, microRecall, microF1\n",
    "\n",
    "\n",
    "#Bidirectional LSTM Model\n",
    "def model1(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Bidirectional(LSTM(LSTM_DIM)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#GRU Model\n",
    "def model2(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(GRU(128))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES * 8, activation='relu'))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#CNN Model\n",
    "def model3(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Conv1D(64, 3, padding='same'))\n",
    "    model.add(Conv1D(32, 3, padding='same'))\n",
    "    model.add(Conv1D(16, 3, padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(180, activation='relu'))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#CNN-LSTM Model\n",
    "def model4(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(LSTM(LSTM_DIM))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#LSTM-CNN Model\n",
    "def model5(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(LSTM(LSTM_DIM, return_sequences=True))\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#CNN-BiLSTM Model\n",
    "def model6(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Bidirectional(LSTM(LSTM_DIM)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#BiLSTM-CNN Model\n",
    "def model7(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Bidirectional(LSTM(LSTM_DIM, return_sequences=True)))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#BiLSTM Model without dropout after embedding layer\n",
    "def model8(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Bidirectional(LSTM(LSTM_DIM, dropout=0.6)))\n",
    "    model.add(Dropout(0.9))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dropout(0.9))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#CNN-GRU Model\n",
    "def model9(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(GRU(LSTM_DIM))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "#GRU-CNN Model\n",
    "def model10(embeddingMatrix):\n",
    "    embeddingLayer = Embedding(embeddingMatrix.shape[0],\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embeddingMatrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    model = Sequential()\n",
    "    model.add(embeddingLayer)\n",
    "    model.add(GRU(LSTM_DIM, return_sequences=True))\n",
    "    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaish\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaish\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "# %load preprocessing.py\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english')) - set(('not', 'no'))\n",
    "\n",
    "\n",
    "tags = ['<url>', '<email>', '<user>', '<hashtag>', '</hashtag>',\n",
    "        '<elongated>', '</elongated>', '<repeated>', '</repeated>']\n",
    "\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'user'],\n",
    "    annotate={'hashtag', 'elongated', 'repeated'},\n",
    "    segmenter=\"twitter\",\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,\n",
    "    unpack_contractions=True,\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    txt = text_processor.pre_process_doc(text)\n",
    "    return list(filter(lambda x: x not in tags and\n",
    "                                 x not in stopwords and\n",
    "                                 x not in punctuation, txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load load.py\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "label2emotion = {0:\"others\", 1:\"happy\", 2:\"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}\n",
    "\n",
    "emotion2label_angry = {\"others\":0, \"happy\":0, \"sad\":0, \"angry\":3}\n",
    "emotion2label_sad = {\"others\":0, \"happy\":0, \"sad\":2, \"angry\":0}\n",
    "emotion2label_happy = {\"others\":0, \"happy\":1, \"sad\":0, \"angry\":0}\n",
    "\n",
    "\n",
    "def load_data(path, training):\n",
    "    data = pd.read_csv(path, encoding='utf-8', sep='\\t')\n",
    "    text = data[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "    if not training:\n",
    "        return data['id'], text\n",
    "    else:\n",
    "        return data['id'], text, data['label']\n",
    "def load_preprocessed_data(path, training=True):\n",
    "    if not training:\n",
    "        id, text = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        return id.values.tolist(), t.values.tolist()\n",
    "    else:\n",
    "        id, text, label = load_data(path, training)\n",
    "        t = text.apply(lambda x: preprocess(x))\n",
    "        l = label.apply(lambda x: emotion2label[x])\n",
    "        return id.values.tolist(), t.values.tolist(), l.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Processing test data...\n",
      "Processing validation data...\n",
      "Extracting tokens...\n",
      "Found 14162 unique tokens.\n",
      "Populating embedding matrix...\n",
      "Found 2196016 word vectors.\n",
      "Building model...\n",
      "WARNING:tensorflow:From C:\\Users\\vaish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\vaish\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 300)           4248900   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64, 300)           540900    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 64, 32)            28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,822,732\n",
      "Trainable params: 4,822,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\vaish\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 30160 samples, validate on 2755 samples\n",
      "Epoch 1/10\n",
      "30160/30160 [==============================] - 152s 5ms/step - loss: 0.7310 - acc: 0.7133 - val_loss: 0.3568 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35680, saving model to ./model1.h5\n",
      "True Positives per class :  [2100.   99.   99.  128.]\n",
      "False Positives per class :  [ 79.  68.  82. 100.]\n",
      "False Negatives per class :  [238.  43.  26.  22.]\n",
      "Class happy : Precision : 0.593, Recall : 0.697, F1 : 0.641\n",
      "Class sad : Precision : 0.547, Recall : 0.792, F1 : 0.647\n",
      "Class angry : Precision : 0.561, Recall : 0.853, F1 : 0.677\n",
      "Ignoring the Others class, Macro Precision : 0.5671, Macro Recall : 0.7808, Macro F1 : 0.6570\n",
      "Ignoring the Others class, Micro TP : 326, FP : 250, FN : 91\n",
      "Accuracy : 0.8806, Micro Precision : 0.5660, Micro Recall : 0.7818, Micro F1 : 0.6566\n",
      "Epoch 2/10\n",
      "30160/30160 [==============================] - 191s 6ms/step - loss: 0.3952 - acc: 0.8644 - val_loss: 0.3724 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.35680\n",
      "True Positives per class :  [2063.  103.  105.  126.]\n",
      "False Positives per class :  [ 73.  68. 123.  94.]\n",
      "False Negatives per class :  [275.  39.  20.  24.]\n",
      "Class happy : Precision : 0.602, Recall : 0.725, F1 : 0.658\n",
      "Class sad : Precision : 0.461, Recall : 0.840, F1 : 0.595\n",
      "Class angry : Precision : 0.573, Recall : 0.840, F1 : 0.681\n",
      "Ignoring the Others class, Macro Precision : 0.5452, Macro Recall : 0.8018, Macro F1 : 0.6491\n",
      "Ignoring the Others class, Micro TP : 334, FP : 285, FN : 83\n",
      "Accuracy : 0.8701, Micro Precision : 0.5396, Micro Recall : 0.8010, Micro F1 : 0.6448\n",
      "Epoch 3/10\n",
      "30160/30160 [==============================] - 194s 6ms/step - loss: 0.3175 - acc: 0.8930 - val_loss: 0.3427 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35680 to 0.34267, saving model to ./model1.h5\n",
      "True Positives per class :  [2092.  111.   97.  123.]\n",
      "False Positives per class :  [ 76. 106.  77.  73.]\n",
      "False Negatives per class :  [246.  31.  28.  27.]\n",
      "Class happy : Precision : 0.512, Recall : 0.782, F1 : 0.618\n",
      "Class sad : Precision : 0.557, Recall : 0.776, F1 : 0.649\n",
      "Class angry : Precision : 0.628, Recall : 0.820, F1 : 0.711\n",
      "Ignoring the Others class, Macro Precision : 0.5655, Macro Recall : 0.7926, Macro F1 : 0.6601\n",
      "Ignoring the Others class, Micro TP : 331, FP : 256, FN : 86\n",
      "Accuracy : 0.8795, Micro Precision : 0.5639, Micro Recall : 0.7938, Micro F1 : 0.6594\n",
      "Epoch 4/10\n",
      "30160/30160 [==============================] - 203s 7ms/step - loss: 0.2577 - acc: 0.9120 - val_loss: 0.3346 - val_acc: 0.8904\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34267 to 0.33456, saving model to ./model1.h5\n",
      "True Positives per class :  [2129.   98.   98.  128.]\n",
      "False Positives per class :  [83. 67. 60. 92.]\n",
      "False Negatives per class :  [209.  44.  27.  22.]\n",
      "Class happy : Precision : 0.594, Recall : 0.690, F1 : 0.638\n",
      "Class sad : Precision : 0.620, Recall : 0.784, F1 : 0.693\n",
      "Class angry : Precision : 0.582, Recall : 0.853, F1 : 0.692\n",
      "Ignoring the Others class, Macro Precision : 0.5987, Macro Recall : 0.7758, Macro F1 : 0.6758\n",
      "Ignoring the Others class, Micro TP : 324, FP : 219, FN : 93\n",
      "Accuracy : 0.8904, Micro Precision : 0.5967, Micro Recall : 0.7770, Micro F1 : 0.6750\n",
      "Epoch 5/10\n",
      "30160/30160 [==============================] - 167s 6ms/step - loss: 0.2073 - acc: 0.9313 - val_loss: 0.3756 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.33456\n",
      "True Positives per class :  [2098.  104.   96.  122.]\n",
      "False Positives per class :  [82. 83. 76. 94.]\n",
      "False Negatives per class :  [240.  38.  29.  28.]\n",
      "Class happy : Precision : 0.556, Recall : 0.732, F1 : 0.632\n",
      "Class sad : Precision : 0.558, Recall : 0.768, F1 : 0.646\n",
      "Class angry : Precision : 0.565, Recall : 0.813, F1 : 0.667\n",
      "Ignoring the Others class, Macro Precision : 0.5597, Macro Recall : 0.7712, Macro F1 : 0.6487\n",
      "Ignoring the Others class, Micro TP : 322, FP : 253, FN : 95\n",
      "Accuracy : 0.8784, Micro Precision : 0.5600, Micro Recall : 0.7722, Micro F1 : 0.6492\n",
      "Epoch 6/10\n",
      "30160/30160 [==============================] - 146s 5ms/step - loss: 0.1631 - acc: 0.9463 - val_loss: 0.4390 - val_acc: 0.8632\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.33456\n",
      "True Positives per class :  [2055.  107.   97.  119.]\n",
      "False Positives per class :  [ 78. 106.  89. 104.]\n",
      "False Negatives per class :  [283.  35.  28.  31.]\n",
      "Class happy : Precision : 0.502, Recall : 0.754, F1 : 0.603\n",
      "Class sad : Precision : 0.522, Recall : 0.776, F1 : 0.624\n",
      "Class angry : Precision : 0.534, Recall : 0.793, F1 : 0.638\n",
      "Ignoring the Others class, Macro Precision : 0.5192, Macro Recall : 0.7743, Macro F1 : 0.6216\n",
      "Ignoring the Others class, Micro TP : 323, FP : 299, FN : 94\n",
      "Accuracy : 0.8632, Micro Precision : 0.5193, Micro Recall : 0.7746, Micro F1 : 0.6218\n",
      "Creating solution file...\n",
      "Completed. Model parameters: \n",
      "Learning rate : 0.001, LSTM Dim : 300, Dropout : 0.200, Batch_size : 200\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "trainIndices, text_train, labels = load_preprocessed_data(trainDataPath)\n",
    "print(\"Processing test data...\")\n",
    "_, text_test = load_preprocessed_data(testDataPath, training=False)\n",
    "print(\"Processing validation data...\")\n",
    "_, X_validation, y_validation = load_preprocessed_data(validationDataPath)\n",
    "\n",
    "print(\"Extracting tokens...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "u_trainSequences = tokenizer.texts_to_sequences(text_train)\n",
    "u_testSequences = tokenizer.texts_to_sequences(text_test)\n",
    "u_validationSequences = tokenizer.texts_to_sequences(X_validation)\n",
    "\n",
    "wordIndex = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(wordIndex))\n",
    "\n",
    "print(\"Populating embedding matrix...\")\n",
    "embeddingMatrix = getEmbeddingMatrix(wordIndex)\n",
    "u_data = pad_sequences(u_trainSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "u_validation = pad_sequences(u_validationSequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_validation = to_categorical(np.asarray(y_validation))\n",
    "\n",
    "np.random.shuffle(trainIndices)\n",
    "u_data = u_data[trainIndices]\n",
    "labels = labels[trainIndices]\n",
    "\n",
    "print(\"Building model...\")\n",
    "cbks = [ModelCheckpoint('./model1.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='auto'),\n",
    "        EarlyStopping(monitor='val_loss', patience=2),\n",
    "        TestCallback((u_validation, labels_validation))]\n",
    "model = model10(embeddingMatrix)\n",
    "model.fit(u_data, labels, validation_data=(u_validation, labels_validation), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, callbacks=cbks)\n",
    "model = load_model('./model1.h5')\n",
    "print(\"Creating solution file...\")\n",
    "create_solution_file(model, u_testSequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
