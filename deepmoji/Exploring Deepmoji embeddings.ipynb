{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "\n",
    "from sys import path as pylib #im naming it as pylib so that we won't get confused between os.path and sys.path \n",
    "import os\n",
    "pylib += [os.path.abspath(r'/home/Jay/Notebooks/Group9_emotion_detection/torchMoji')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../deepmoji/data/train.txt', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    }
   ],
   "source": [
    "label2emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
    "emotion2label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}\n",
    "\n",
    "emoticons_additional = {\n",
    "    '(^・^)': '<happy>', ':‑c': '<sad>', '=‑d': '<happy>', \":'‑)\": '<happy>', ':‑d': '<laugh>',\n",
    "    ':‑(': '<sad>', ';‑)': '<happy>', ':‑)': '<happy>', ':\\\\/': '<sad>', 'd=<': '<annoyed>',\n",
    "    ':‑/': '<annoyed>', ';‑]': '<happy>', '(^�^)': '<happy>', 'angru': 'angry', \"d‑':\":\n",
    "        '<annoyed>', \":'‑(\": '<sad>', \":‑[\": '<annoyed>', '(�?�)': '<happy>', 'x‑d': '<laugh>',\n",
    "}\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "               'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "              'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\",\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons, emoticons_additional]\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(text_processor.pre_process_doc(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=[]\n",
    "x2=[]\n",
    "x3=[]\n",
    "y=[]\n",
    "labels={}\n",
    "i=0\n",
    "for ind, row in data.iterrows():\n",
    "    if row['label'] not in labels:\n",
    "        labels[row['label']]=i\n",
    "        i+=1\n",
    "    y.append(labels[row['label']])\n",
    "    x1.append(tokenize(row['turn1']))\n",
    "    x2.append(tokenize(row['turn2']))\n",
    "    x3.append(tokenize(row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../../deepmoji/data/test.txt', sep = '\\t')\n",
    "test_x1=[]\n",
    "test_x2=[]\n",
    "test_x3=[]\n",
    "test_y=[]\n",
    "test_labels={}\n",
    "i=0\n",
    "for ind, row in test_data.iterrows():\n",
    "    if row['label'] not in test_labels:\n",
    "        test_labels[row['label']]=i\n",
    "        i+=1\n",
    "    test_y.append(test_labels[row['label']])\n",
    "    test_x1.append(tokenize(row['turn1']))\n",
    "    test_x2.append(tokenize(row['turn2']))\n",
    "    test_x3.append(tokenize(row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import json\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_feature_encoding\n",
    "from torchmoji.model_def import torchmoji_emojis\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "def encode_emoji(x):\n",
    "    model = torchmoji_emojis(PRETRAINED_PATH)\n",
    "    maxlen = 30\n",
    "    batch_size = 32\n",
    "\n",
    "    print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "    with open(VOCAB_PATH, 'r') as f:\n",
    "        vocabulary = json.load(f)\n",
    "    st = SentenceTokenizer(vocabulary, maxlen)\n",
    "    tokenized, _, _ = st.tokenize_sentences(x)\n",
    "    print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "    print(model)\n",
    "    print('Running predictions.')\n",
    "    \n",
    "    encoding = np.zeros((len(x), 64))\n",
    "    for i in range(0, len(x), 300):    \n",
    "        encoding[i:i+300] = model(tokenized[i:i+300])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)\n",
    "idx_others=np.where(y==0)[0]\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "x3 = np.array(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.c_[x1,x2,x3]\n",
    "x_all = np.array([' <eos> \\n '.join(x) for x in x_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_emoji = encode_emoji(x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_emoji = encode_emoji(x1)\n",
    "x2_emoji = encode_emoji(x2)\n",
    "x3_emoji = encode_emoji(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_elements(array, k):\n",
    "    ind = np.argpartition(array, -k)[-k:]\n",
    "    return ind[np.argsort(array[ind])][::-1]\n",
    "\n",
    "EMOJIS = \":joy: :unamused: :weary: :sob: :heart_eyes: \\\n",
    ":pensive: :ok_hand: :blush: :heart: :smirk: \\\n",
    ":grin: :notes: :flushed: :100: :sleeping: \\\n",
    ":relieved: :relaxed: :raised_hands: :two_hearts: :expressionless: \\\n",
    ":sweat_smile: :pray: :confused: :kissing_heart: :heartbeat: \\\n",
    ":neutral_face: :information_desk_person: :disappointed: :see_no_evil: :tired_face: \\\n",
    ":v: :sunglasses: :rage: :thumbsup: :cry: \\\n",
    ":sleepy: :yum: :triumph: :hand: :mask: \\\n",
    ":clap: :eyes: :gun: :persevere: :smiling_imp: \\\n",
    ":sweat: :broken_heart: :yellow_heart: :musical_note: :speak_no_evil: \\\n",
    ":wink: :skull: :confounded: :smile: :stuck_out_tongue_winking_eye: \\\n",
    ":angry: :no_good: :muscle: :facepunch: :purple_heart: \\\n",
    ":sparkling_heart: :blue_heart: :grimacing: :sparkles:\".split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn1=[]\n",
    "turn2=[]\n",
    "turn3=[]\n",
    "\n",
    "for i, text in enumerate(data['turn1'].values):\n",
    "    emoji_ids = top_elements(x1_emoji[i], 5)\n",
    "    emojis = map(lambda x: EMOJIS[x], emoji_ids)\n",
    "    turn1.append(emoji.emojize(\"{} {}\".format(text,' '.join(emojis)), use_aliases=True))\n",
    "\n",
    "for i, text in enumerate(data['turn2'].values):\n",
    "    emoji_ids = top_elements(x2_emoji[i], 5)\n",
    "    emojis = map(lambda x: EMOJIS[x], emoji_ids)\n",
    "    turn2.append(emoji.emojize(\"{} {}\".format(text,' '.join(emojis)), use_aliases=True))\n",
    "\n",
    "for i, text in enumerate(data['turn3'].values):\n",
    "    emoji_ids = top_elements(x3_emoji[i], 5)\n",
    "    emojis = map(lambda x: EMOJIS[x], emoji_ids)\n",
    "    turn3.append(emoji.emojize(\"{} {}\".format(text,' '.join(emojis)), use_aliases=True))\n",
    "    #print(emoji.emojize(\"{} {}\".format(x1[i],' '.join(emojis)), use_aliases=True))\n",
    "\n",
    "data['turn1'] = turn1\n",
    "data['turn2'] = turn2\n",
    "data['turn3'] = turn3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i, prob in enumerate(x_emoji):\n",
    "    emoji_ids = top_elements(prob, 5)\n",
    "    emojis = map(lambda x: EMOJIS[x], emoji_ids)\n",
    "    labels.append(emoji.emojize(' '.join(emojis), use_aliases=True))\n",
    "    \n",
    "data['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_others = data[data['label']=='others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_others"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
