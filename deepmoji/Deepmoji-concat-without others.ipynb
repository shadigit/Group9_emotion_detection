{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sys import path as pylib #im naming it as pylib so that we won't get confused between os.path and sys.path \n",
    "import os\n",
    "pylib += [os.path.abspath(r'/home/Jay/Notebooks/Group9_emotion_detection/torchMoji')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../deepmoji/data/train.txt', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "label2emotion = {0: \"others\", 1: \"happy\", 2: \"sad\", 3: \"angry\"}\n",
    "emotion2label = {\"others\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3}\n",
    "\n",
    "emoticons_additional = {\n",
    "    '(^・^)': '<happy>', ':‑c': '<sad>', '=‑d': '<happy>', \":'‑)\": '<happy>', ':‑d': '<laugh>',\n",
    "    ':‑(': '<sad>', ';‑)': '<happy>', ':‑)': '<happy>', ':\\\\/': '<sad>', 'd=<': '<annoyed>',\n",
    "    ':‑/': '<annoyed>', ';‑]': '<happy>', '(^�^)': '<happy>', 'angru': 'angry', \"d‑':\":\n",
    "        '<annoyed>', \":'‑(\": '<sad>', \":‑[\": '<annoyed>', '(�?�)': '<happy>', 'x‑d': '<laugh>',\n",
    "}\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "               'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "              'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\",\n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\",\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons, emoticons_additional]\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \" \".join(text_processor.pre_process_doc(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "labels={}\n",
    "i=0\n",
    "for ind, row in data.iterrows():\n",
    "    if row['label'] not in labels:\n",
    "        labels[row['label']]=i\n",
    "        i+=1\n",
    "    y.append(labels[row['label']])\n",
    "    x.append(tokenize(row['turn1'])+' <eos> '+tokenize(row['turn2']) + ' <eos> '+tokenize(row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../../deepmoji/data/test.txt', sep = '\\t')\n",
    "test_x = []\n",
    "test_y=[]\n",
    "test_labels={}\n",
    "i=0\n",
    "for ind, row in test_data.iterrows():\n",
    "    if row['label'] not in test_labels:\n",
    "        test_labels[row['label']]=i\n",
    "        i+=1\n",
    "    test_y.append(test_labels[row['label']])\n",
    "    test_x.append(tokenize(row['turn1'])+' <eos> '+tokenize(row['turn2']) + ' <eos> '+tokenize(row['turn3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import json\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_feature_encoding\n",
    "from torchmoji.model_def import torchmoji_emojis\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "def encode_deepmoji(x):\n",
    "    maxlen = 30\n",
    "    batch_size = 32\n",
    "\n",
    "    print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "    with open(VOCAB_PATH, 'r') as f:\n",
    "        vocabulary = json.load(f)\n",
    "    st = SentenceTokenizer(vocabulary, maxlen)\n",
    "    tokenized, _, _ = st.tokenize_sentences(x)\n",
    "    print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "    model = torchmoji_feature_encoding(PRETRAINED_PATH)\n",
    "    print(model)\n",
    "    print('Encoding texts..')\n",
    "    encoding = np.zeros((len(x), 2304))\n",
    "    for i in range(0, len(x), 300):    \n",
    "        encoding[i:i+300] = model(tokenized[i:i+300])\n",
    "    return encoding\n",
    "\n",
    "def encode_emoji(x):\n",
    "    model = torchmoji_emojis(PRETRAINED_PATH)\n",
    "    maxlen = 30\n",
    "    batch_size = 32\n",
    "\n",
    "    print('Tokenizing using dictionary from {}'.format(VOCAB_PATH))\n",
    "    with open(VOCAB_PATH, 'r') as f:\n",
    "        vocabulary = json.load(f)\n",
    "    st = SentenceTokenizer(vocabulary, maxlen)\n",
    "    tokenized, _, _ = st.tokenize_sentences(x)\n",
    "    print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "    print(model)\n",
    "    print('Running predictions.')\n",
    "    \n",
    "    encoding = np.zeros((len(x), 64))\n",
    "    for i in range(0, len(x), 300):    \n",
    "        encoding[i:i+300] = model(tokenized[i:i+300])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec = np.load('../../dm_emb/deepmoji_train_x.npy')\n",
    "test_x_vec = np.load('../../dm_emb/deepmoji_test_x.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30160, 2304)\n",
      "(5509, 2304)\n"
     ]
    }
   ],
   "source": [
    "print(x_vec.shape)\n",
    "print(test_x_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing using dictionary from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/vocabulary.json\n",
      "Loading model from /home/Jay/Notebooks/Group9_emotion_detection/torchMoji/model/pytorch_model.bin.\n",
      "TorchMoji(\n",
      "  (embed): Embedding(50000, 256)\n",
      "  (embed_dropout): Dropout2d(p=0)\n",
      "  (lstm_0): LSTMHardSigmoid(256, 512, batch_first=True, bidirectional=True)\n",
      "  (lstm_1): LSTMHardSigmoid(1024, 512, batch_first=True, bidirectional=True)\n",
      "  (attention_layer): Attention(2304, return attention=False)\n",
      "  (final_dropout): Dropout(p=0)\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=2304, out_features=64, bias=True)\n",
      "    (1): Softmax()\n",
      "  )\n",
      ")\n",
      "Running predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Jay/dev32/lib/python3.5/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "emoji_all = encode_emoji(x)\n",
    "emoji_test = encode_emoji(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [v for k,v in label2emotion.items()]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)\n",
    "idx_others=np.where(y==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.r_[np.where(y==1)[0],np.where(y==2)[0],np.where(y==3)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepmoji_train = x_vec[idx]\n",
    "y_train = y[idx]\n",
    "emoji_train = emoji_all[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepmoji_test = test_x_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(test_y)\n",
    "idx_test = np.r_[np.where(test_y==1)[0],np.where(test_y==2)[0],np.where(test_y==3)[0]]\n",
    "test_y = test_y[idx_test]\n",
    "deepmoji_test = deepmoji_test[idx_test]\n",
    "emoji_test = emoji_test[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def print_metrics(mc_prediction):\n",
    "    print(metrics.classification_report(test_y, mc_prediction, target_names=tokens[1:]))\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(test_y, mc_prediction))\n",
    "    print(\"Precision:\", metrics.precision_score(test_y, mc_prediction, average='macro'))\n",
    "    print(\"Recall:\", metrics.recall_score(test_y, mc_prediction, average='macro'))\n",
    "    print(\"F1 score: \", metrics.f1_score(test_y, mc_prediction, average='macro'))\n",
    "    #print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(mc_Y_test, mc_prediction))\n",
    "    plot_confusion_matrix(metrics.confusion_matrix(test_y, mc_prediction), tokens[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = Pipeline([('scaler', MinMaxScaler()), ('clf', LogisticRegression(random_state=0))])\n",
    "#LR = LogisticRegression(random_state=0)\n",
    "LR.fit(deepmoji_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(LR.predict(deepmoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=0)\n",
    "LR.fit(deepmoji_train, y_train) \n",
    "print_metrics(LR.predict(deepmoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=0).fit(x_vec, y) \n",
    "print_metrics(LR.predict(deepmoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=0).fit(emoji_train, y_train) \n",
    "print_metrics(LR.predict(emoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=0).fit(emoji_all, y) \n",
    "print_metrics(LR.predict(emoji_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_nb = Pipeline([('scaler', MinMaxScaler()), ('clf', MultinomialNB())])\n",
    "mc_nb.fit(deepmoji_train, y_train)\n",
    "print_metrics(mc_nb.predict(deepmoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_nb = Pipeline([('clf', MultinomialNB())])\n",
    "mc_nb.fit(emoji_train, y_train)\n",
    "print_metrics(mc_nb.predict(emoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_nb.fit(emoji_all, y)\n",
    "print_metrics(mc_nb.predict(emoji_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - One vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_svm_ovo = svm.SVC(decision_function_shape='ovo', kernel='linear', C=100)\n",
    "\n",
    "mc_svm_ovo.fit(emoji_train, y_train)\n",
    "print_metrics(mc_svm_ovo.predict(emoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_svm_ovo.fit(emoji_all, y)\n",
    "print_metrics(mc_svm_ovo.predict(emoji_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM One vs Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_svm_ovr = svm.SVC(decision_function_shape='ovr', kernel='linear', C=100)\n",
    "mc_svm_ovr.fit(emoji_train, y_train)\n",
    "print_metrics(mc_svm_ovr.predict(emoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_svm_ovr.fit(emoji_all, y)\n",
    "print_metrics(mc_svm_ovr.predict(emoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
